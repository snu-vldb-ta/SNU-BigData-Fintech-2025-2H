{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4UHZzoV_rw2"
      },
      "source": [
        "# NL2SQL Practice 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yN1quCVZlo8"
      },
      "outputs": [],
      "source": [
        "!unzip /content/data.zip -d /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "76fMtIPBRcoY"
      },
      "outputs": [],
      "source": [
        "pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2K3x8W_f8hOL"
      },
      "source": [
        "## OpenAI API Connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-2tWWSzXU8A"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "from openai import OpenAI\n",
        "\n",
        "import chromadb\n",
        "import chromadb.utils.embedding_functions as embedding_functions\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "OPENAI_API_KEY = getpass(\"Please enter your OpenAI API key: \")\n",
        "\n",
        "llm = OpenAI(api_key=OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiBRY8Pm-oSf"
      },
      "source": [
        "## Storing Data in Knowledge Databases (VectorDB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9dbQIH7Dxen_"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "\n",
        "# Load schema information file\n",
        "with open(\"superhero.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    schema_data = json.load(f)\n",
        "\n",
        "# Prepare documents and metadata\n",
        "documents = [] # list of contents that will be transformed into embedding vectors (\"{column} + {description}\")\n",
        "metadatas = [] # list of metadata that will not be transformed into embedding vectors, but still linked with the embedding vectors\n",
        "ids = [] # (id for each data stored in vectordb should be in \"text\" format for current version of ChromaDB)\n",
        "\n",
        "for table in schema_data:\n",
        "    table_name = table[\"table\"]\n",
        "    for col in table[\"column\"]:\n",
        "        col_name = col[\"column_name\"]\n",
        "        description = col.get(\"description\", \"\").lstrip(\"#\").strip()\n",
        "        document = f\"column_name: {col_name}\\n{description}\"\n",
        "        # e.g., \"column_name: colour The colour column in the superhero table represents the color of the superhero's skin/eye/hair/etc.\"\n",
        "\n",
        "        documents.append(document)\n",
        "        metadatas.append({\n",
        "            \"table\": table_name,\n",
        "            \"column\": col_name\n",
        "        })\n",
        "        ids.append(f\"col-{table_name}-{col_name}\")\n",
        "\n",
        "# ChromaDB setup\n",
        "client = chromadb.PersistentClient(path=\"./vectordb/\")\n",
        "\n",
        "# Use OpenAI embedding model for embedding vector transformation.\n",
        "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
        "    api_key=OPENAI_API_KEY,\n",
        "    model_name=\"text-embedding-3-small\"\n",
        ")\n",
        "\n",
        "# Create collection\n",
        "collection = client.get_or_create_collection(\"column_description\", embedding_function=openai_ef)\n",
        "\n",
        "# Save the data in collection\n",
        "collection.add(\n",
        "    documents=documents,\n",
        "    metadatas=metadatas,\n",
        "    ids=ids\n",
        ")\n",
        "\n",
        "print(\"Column descriptions saved to ChromaDB!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7mBDp4XLXVbF"
      },
      "outputs": [],
      "source": [
        "# Json file storing few-shot exmamples\n",
        "jsonl_file = \"fewshot.jsonl\"\n",
        "\n",
        "# Open Json file -> Pandas DataFrame\n",
        "records = []\n",
        "with open(jsonl_file, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        records.append(json.loads(line))\n",
        "\n",
        "df = pd.DataFrame(records)\n",
        "\n",
        "client = chromadb.PersistentClient(path=\"./vectordb/\")\n",
        "\n",
        "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
        "    api_key=OPENAI_API_KEY,\n",
        "    model_name=\"text-embedding-3-small\"\n",
        ")\n",
        "\n",
        "collection = client.get_or_create_collection(\"fewshot\", embedding_function=openai_ef)\n",
        "\n",
        "documents = df[\"question\"].tolist() #\n",
        "metadatas = df[[\"evidence\", \"question\", \"SQL\"]].to_dict(orient=\"records\")\n",
        "ids = [\"few-shot-\" + str(qid) for qid in df[\"question_id\"]]\n",
        "\n",
        "collection.add(\n",
        "    documents=documents,\n",
        "    metadatas=metadatas,\n",
        "    ids=ids\n",
        ")\n",
        "\n",
        "print(\"All few-shot examples saved in ChromaDB!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0rmh7If_8sn"
      },
      "source": [
        "## Text-to-SQL using OpenAI LLM (GPT-4o-mini)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABcsAX2h0p1W"
      },
      "outputs": [],
      "source": [
        "# user_question / gold SQL from BIRD dev.\n",
        "with open(\"sample.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    sample_data = json.load(f)\n",
        "\n",
        "# first sample_data (sample_data[1]: second sample_data)\n",
        "user_question = sample_data[0]['question']\n",
        "knowledge_evidence = sample_data[0]['evidence']\n",
        "gold_SQL = sample_data[0]['SQL'] # for evaluation\n",
        "\n",
        "print(\"Question: \", user_question)\n",
        "print(\"Hint: \", knowledge_evidence)\n",
        "print(\"Gold SQL: \", gold_SQL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hknc2jmOApF4"
      },
      "source": [
        "### Schema Linking (Get relevant columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddaqhpm6xz8X"
      },
      "outputs": [],
      "source": [
        "def find_relevant_columns(query, top_k=5):\n",
        "    client = chromadb.PersistentClient(path=\"./vectordb/\")\n",
        "\n",
        "    openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
        "        api_key=OPENAI_API_KEY,\n",
        "        model_name=\"text-embedding-3-small\" # use same embedding model with one that were used for storing data in vectorDB\n",
        "    )\n",
        "\n",
        "    collection = client.get_or_create_collection(\"column_description\", embedding_function=openai_ef)\n",
        "\n",
        "    # Similarity Search\n",
        "    results = collection.query(\n",
        "        query_texts=[query], # batch available\n",
        "        n_results=top_k # retrieve top k relevant data\n",
        "    )\n",
        "\n",
        "    print(f\"\\nüîç Top {top_k} columns relevant to:\\n\\\"{query}\\\"\\n\")\n",
        "\n",
        "    for doc, meta, score in zip(results['documents'][0], results['metadatas'][0], results['distances'][0]):\n",
        "        print(f\"# Table: {meta['table']}, Column: {meta['column']}\")\n",
        "        print(f\"# Description: {doc.splitlines()[1]}\")\n",
        "        print(f\"# Score: {score:.4f}\\n\")\n",
        "\n",
        "    return results['metadatas'][0]  # Î¶¨Ïä§Ìä∏ of {table, column}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrwIfkczWe1d"
      },
      "outputs": [],
      "source": [
        "relevant_columns = find_relevant_columns(user_question, top_k=5)\n",
        "print(relevant_columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h70l4_NSBUpj"
      },
      "source": [
        "### Schema Representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTzXJ8Srro-c"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import json\n",
        "\n",
        "def generate_filtered_schema(input_file, used_columns=None, include_descriptions=False):\n",
        "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        tables = json.load(f)\n",
        "\n",
        "    output_lines = []\n",
        "\n",
        "    if used_columns: # use schema selected from schema-linking\n",
        "        used_set = set((item[\"table\"], item[\"column\"]) for item in used_columns)\n",
        "    else:\n",
        "        used_set = None  # use full schema instead of selected subset\n",
        "\n",
        "    for table in tables:\n",
        "        table_name = table[\"table\"]\n",
        "\n",
        "        if used_set:\n",
        "            table_columns = [col for col in table[\"column\"] if (table_name, col[\"column_name\"]) in used_set]\n",
        "            if not table_columns:\n",
        "                continue\n",
        "        else:\n",
        "            table_columns = table[\"column\"]\n",
        "\n",
        "        output_lines.append(f\"Table: {table_name}\")\n",
        "\n",
        "        for col in table_columns:\n",
        "            col_name = col[\"column_name\"]\n",
        "            is_pk = col[\"PK\"] == 1\n",
        "            fk = col[\"FK\"]\n",
        "            desc = col.get(\"description\", None)\n",
        "\n",
        "            if is_pk:\n",
        "                output_lines.append(f\"- Column (PK): {col_name}\")\n",
        "            elif fk:\n",
        "                output_lines.append(f\"- Column (FK): {col_name}\")\n",
        "            else:\n",
        "                output_lines.append(f\"- Column: {col_name}\")\n",
        "\n",
        "            # include column descriptions in prompt?\n",
        "            if include_descriptions:\n",
        "                if desc:\n",
        "                    cleaned = desc.strip().lstrip(\"#\").strip()\n",
        "                    output_lines.append(f\"  - Description: {cleaned}\")\n",
        "\n",
        "        output_lines.append(\"\")\n",
        "\n",
        "    return \"\\n\".join(output_lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rR03zJtfBmIk"
      },
      "outputs": [],
      "source": [
        "relevant_columns = find_relevant_columns(user_question, top_k=5)\n",
        "\n",
        "schema_info = generate_filtered_schema(\n",
        "    input_file=\"superhero.json\",\n",
        "    used_columns=None, # relevant_columns\n",
        "    include_descriptions=True #\n",
        ")\n",
        "\n",
        "print(schema_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgAX9w1ZAvrH"
      },
      "source": [
        "### Few-shot Retrieval (Get relevant examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjkOzBId1Voa"
      },
      "outputs": [],
      "source": [
        "def get_relevant_fewshots(user_question, top_k=3):\n",
        "    import chromadb\n",
        "    from chromadb.utils import embedding_functions\n",
        "\n",
        "    client = chromadb.PersistentClient(path=\"./vectordb/\")\n",
        "    openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
        "        api_key=OPENAI_API_KEY,\n",
        "        model_name=\"text-embedding-3-small\"\n",
        "    )\n",
        "    collection = client.get_or_create_collection(\"fewshot\", embedding_function=openai_ef)\n",
        "\n",
        "    results = collection.query(\n",
        "        query_texts=[user_question],\n",
        "        n_results=top_k\n",
        "    )\n",
        "\n",
        "    fewshot_blocks = []\n",
        "    for idx, (doc, meta) in enumerate(zip(results[\"documents\"][0], results[\"metadatas\"][0]), 1):\n",
        "        block = f\"\"\"### Example {idx}\n",
        "Question: {meta['question']}\n",
        "Evidence: {meta['evidence']}\n",
        "SQL:\n",
        "{meta['SQL']}\"\"\"\n",
        "        fewshot_blocks.append(block)\n",
        "\n",
        "    return \"\\n\\n\".join(fewshot_blocks)\n",
        "\n",
        "fewshots_text = get_relevant_fewshots(user_question, top_k=3)\n",
        "\n",
        "print(fewshots_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FltWB6M5BrE5"
      },
      "source": [
        "### Prompt Template for SQL Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "welHXUyAqng3"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"You are a data science expert.\n",
        "Below, you are presented with a database schema and a question.\n",
        "Your task is to read the schema, understand the question, and generate a valid SQLite query to answer the question.\n",
        "Before generating the final SQL query think step by step on how to write the query.\n",
        "\n",
        "Database Schema:\n",
        "{DATABASE_SCHEMA}\n",
        "\n",
        "Few-shot Examples:\n",
        "{FEWSHOTS}\n",
        "\n",
        "This schema offers an in-depth description of the database‚Äôs architecture, detailing tables, columns, primary keys, foreign keys, and any pertinent information regarding relationships or constraints. Special attention should be given to the examples listed beside each column, as they directly hint at which columns are relevant to our query.\n",
        "\n",
        "Database admin instructions:\n",
        "- Make sure you only output the information that is asked in the question. If the question asks for a specific column, make sure to only include that column in the SELECT clause, nothing more.\n",
        "- Predicted query should return all of the information asked in the question without any missing or extra information.\n",
        "\n",
        "Question:\n",
        "{QUESTION}\n",
        "\n",
        "Hint:\n",
        "{HINT}\n",
        "\n",
        "Please respond with a JSON object structured as follows:\n",
        "\n",
        "{{\n",
        "  \"chain_of_thought_reasoning\": \"Your thought process on how you arrived at the final SQL query.\",\n",
        "  \"SQL\": \"Your SQL query in a single string.\"\n",
        "}}\n",
        "\n",
        "Priority should be given to columns that have been explicitly matched with examples relevant to the question‚Äôs context.\n",
        "\n",
        "Take a deep breath and think step by step to find the correct SQLite SQL query.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OWsskMjN-FK"
      },
      "source": [
        "### Generate SQL query using LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsgJalBBf2EM"
      },
      "outputs": [],
      "source": [
        "prompt = prompt_template.format(\n",
        "    DATABASE_SCHEMA=schema_info,\n",
        "    QUESTION=user_question,\n",
        "    HINT=knowledge_evidence, # \"\"\n",
        "    FEWSHOTS=fewshots_text, # \"\"\n",
        ")\n",
        "\n",
        "response = llm.chat.completions.create(\n",
        "        model='gpt-4o-mini',\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "generated_text = response.choices[0].message.content\n",
        "generated_dict = json.loads(generated_text)\n",
        "predicted_SQL = generated_dict[\"SQL\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilpGnxU9vIto"
      },
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "\n",
        "db_path = \"superhero.sqlite\"\n",
        "\n",
        "def run_query(db_path, SQL):\n",
        "  try:\n",
        "      conn = sqlite3.connect(db_path)\n",
        "      cursor = conn.cursor()\n",
        "\n",
        "      cursor.execute(SQL)\n",
        "      results = cursor.fetchall()\n",
        "\n",
        "      # # PRINT RESULT (MIGHT BE REALLY LONG!)\n",
        "      # print(\"Results:\")\n",
        "      # for row in results:\n",
        "      #     print(row)\n",
        "\n",
        "      return results\n",
        "  except Exception as e:\n",
        "      print(\"Error while executing query:\", e)\n",
        "      return None\n",
        "  finally:\n",
        "      conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27Hy2IKkwn7z"
      },
      "outputs": [],
      "source": [
        "def normalize_result(rows):\n",
        "    # set comparison ignoring row/column order\n",
        "    return set(tuple(row) for row in rows)\n",
        "\n",
        "predicted_results = run_query(db_path, predicted_SQL)\n",
        "gold_results = run_query(db_path, gold_SQL)\n",
        "\n",
        "if predicted_results is not None and gold_results is not None:\n",
        "    if normalize_result(predicted_results) == normalize_result(gold_results):\n",
        "        print(\"Prediction matches gold SQL output!\")\n",
        "    else:\n",
        "        print(\"Prediction does NOT match gold SQL output.\")\n",
        "        print(\"Predicted Results:\", predicted_results)\n",
        "        print(\"Gold Results:\", gold_results)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
